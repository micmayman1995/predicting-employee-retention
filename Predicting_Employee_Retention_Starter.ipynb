{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h93yUiuRbzEv"
   },
   "source": [
    "# **Predicting Employee Retention**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7YvL652bzE0"
   },
   "source": [
    "### Objective\n",
    "\n",
    "The objective of this assignment is to develop a Logistic Regression model. You will be using this model to analyse and predict binary outcomes based on the input data. This assignment aims to enhance understanding of logistic regression, including its assumptions, implementation, and evaluation, to effectively classify and interpret data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waZ4IVr4bzE1"
   },
   "source": [
    "### Business Objective\n",
    "\n",
    "A mid-sized technology company wants to improve its understanding of employee retention to foster a loyal and committed workforce. While the organization has traditionally focused on addressing turnover, it recognises the value of proactively identifying employees likely to stay and understanding the factors contributing to their loyalty.\n",
    "\n",
    "\n",
    "In this assignment you’ll be building a logistic regression model to predict the likelihood of employee retention based on the data such as demographic details, job satisfaction scores, performance metrics, and tenure. The aim is to provide the HR department with actionable insights to strengthen retention strategies, create a supportive work environment, and increase the overall stability and satisfaction of the workforce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7zFcptgbzE2"
   },
   "source": [
    "## Assignment Tasks\n",
    "\n",
    "You need to perform the following steps to complete this assignment:\n",
    "1. Data Understanding\n",
    "2. Data Cleaning\n",
    "3. Train Validation Split\n",
    "4. EDA on training data\n",
    "5. EDA on validation data [Optional]\n",
    "6. Feature Engineering\n",
    "7. Model Building\n",
    "8. Prediction and Model Evaluation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejDKL2NDceHc"
   },
   "source": [
    "## Data Dictionary\n",
    "\n",
    "The data has 24 Columns and 74610 Rows. Following data dictionary provides the description for each column present in dataset:<br>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Column Name</th>\n",
    "      <th>Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Employee ID</td>\n",
    "      <td>A unique identifier assigned to each employee.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Age</td>\n",
    "      <td>The age of the employee, ranging from 18 to 60 years.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Gender</td>\n",
    "      <td>The gender of the employee.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Years at Company</td>\n",
    "      <td>The number of years the employee has been working at the company.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Monthly Income</td>\n",
    "      <td>The monthly salary of the employee, in dollars.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Job Role</td>\n",
    "      <td>The department or role the employee works in, encoded into categories such as Finance, Healthcare, Technology, Education, and Media.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Work-Life Balance</td>\n",
    "      <td>The employee's perceived balance between work and personal life (Poor, Below Average, Good, Excellent).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Job Satisfaction</td>\n",
    "      <td>The employee's satisfaction with their job (Very Low, Low, Medium, High).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Performance Rating</td>\n",
    "      <td>The employee's performance rating (Low, Below Average, Average, High).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Number of Promotions</td>\n",
    "      <td>The total number of promotions the employee has received.</td>\n",
    "    </tr>\n",
    "     </tr>\n",
    "     <tr>\n",
    "      <td>Overtime</td>\n",
    "      <td>Number of overtime hours.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Distance from Home</td>\n",
    "      <td>The distance between the employee's home and workplace, in miles.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Education Level</td>\n",
    "      <td>The highest education level attained by the employee (High School, Associate Degree, Bachelor’s Degree, Master’s Degree, PhD).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Marital Status</td>\n",
    "      <td>The marital status of the employee (Divorced, Married, Single).</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "      <td>Number of Dependents</td>\n",
    "      <td>Number of dependents the employee has.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Job Level</td>\n",
    "      <td>The job level of the employee (Entry, Mid, Senior).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Company Size</td>\n",
    "      <td>The size of the company the employee works for (Small, Medium, Large).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Company Tenure (In Months)</td>\n",
    "      <td>The total number of years the employee has been working in the industry.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Remote Work</td>\n",
    "      <td>Whether the employee works remotely (Yes or No).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Leadership Opportunities</td>\n",
    "      <td>Whether the employee has leadership opportunities (Yes or No).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Innovation Opportunities</td>\n",
    "      <td>Whether the employee has opportunities for innovation (Yes or No).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Company Reputation</td>\n",
    "      <td>The employee's perception of the company's reputation (Very Poor, Poor, Good, Excellent).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Employee Recognition</td>\n",
    "      <td>The level of recognition the employee receives(Very Low, Low, Medium, High).</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Attrition</td>\n",
    "      <td>Whether the employee has left the company.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lmfQyH2bzE5"
   },
   "source": [
    "## **1. Data Understanding**\n",
    "\n",
    "In this step, load the dataset and check basic statistics of the data, including preview of data, dimension of data, column descriptions and data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqhCqH69bzE2"
   },
   "source": [
    "### **1.0 Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1bADKAJibzE3"
   },
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vOoaowGFbzE5"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzOBYKpCcJZJ"
   },
   "source": [
    "### **1.1 Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vibm0dYcbzE6"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "employee_raw = pd.read_csv('Employee_data.csv')\n",
    "\n",
    "# Normalize garbled characters in text columns\n",
    "text_cols = employee_raw.select_dtypes(include=\"object\").columns\n",
    "for c in text_cols:\n",
    "    employee_raw[c] = (\n",
    "        employee_raw[c]\n",
    "        .astype(str)\n",
    "        .str.replace(\"â€™\", \"'\", regex=False)\n",
    "        .str.strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVeUFyDxbzE6"
   },
   "outputs": [],
   "source": [
    "# Check the first few entries\n",
    "employee_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msGYpADRbzE7"
   },
   "outputs": [],
   "source": [
    "# Inspect the shape of the dataset\n",
    "employee_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL53eXK6bzE7"
   },
   "outputs": [],
   "source": [
    "# Inspect the different columns in the dataset\n",
    "employee_raw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrFKEWp8cmxB"
   },
   "source": [
    "### **1.2 Check the basic statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCykdG90bzE8"
   },
   "outputs": [],
   "source": [
    "# Check the summary of the dataset\n",
    "employee_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlLfK93PdFkt"
   },
   "source": [
    "### **1.3 Check the data type of columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TplEft3bzE8"
   },
   "outputs": [],
   "source": [
    "# Check the info to see the types of the feature variables and the null values present\n",
    "employee_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNXum35MbzE9"
   },
   "source": [
    "## **2. Data Cleaning** <font color = red>[15 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tC_ik7dobzE9"
   },
   "source": [
    "### **2.1 Handle the missing values** <font color = red>[10 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gPB7iZwdqxM"
   },
   "source": [
    "2.1.1 Check the number of missing values <font color=\"red\">[2 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOQGBvHSbzE9"
   },
   "outputs": [],
   "source": [
    "# Check the number of missing values in each column\n",
    "missing_val_cols = employee_raw.isna().sum().sort_values(ascending=False)\n",
    "missing_val_cols = missing_val_cols[missing_val_cols > 0]\n",
    "missing_val_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcFIVIOlR_91"
   },
   "source": [
    "2.1.2 Check the percentage of missing values <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjpnMExYbzE9"
   },
   "outputs": [],
   "source": [
    "# Check the percentage of missing values in each column\n",
    "missing_val_cols / employee_raw.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJL1T8-Sd6S-"
   },
   "source": [
    "2.1.3 Handle rows with missing values <font color=\"red\">[4 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIMPqTTWbzE_"
   },
   "outputs": [],
   "source": [
    "# Handle the missing value rows in the column\n",
    "\n",
    "employee_clean = employee_raw.copy()\n",
    "for i in missing_val_cols.index:\n",
    "    employee_clean[i] = employee_clean[i].fillna(employee_clean[i].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uefvEMkDeJhv"
   },
   "source": [
    "2.1.4 Check percentage of remaning data after missing values are removed <font color=\"red\">[2 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPIVeWX2bzFA"
   },
   "outputs": [],
   "source": [
    "# Check the percentage of remaining data after missing values are removed\n",
    "employee_clean.isna().sum().sort_values(ascending=False) / employee_clean.shape[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbUOnf6rGEF1"
   },
   "source": [
    "### **2.2 Identify and handle redundant values within categorical columns (if any)** <font color = red>[3 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEYeQw_SeaLi"
   },
   "source": [
    "Examine the categorical columns to determine if any value or column needs to be treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIBKtZhZjIk6"
   },
   "outputs": [],
   "source": [
    "# Write a function to display the categorical columns with their unique values and check for redundant values\n",
    "\n",
    "def show_categoricals(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Display categorical columns with their unique values and check for redundant values\n",
    "    (case-insensitive and whitespace-normalized).\n",
    "    \"\"\"\n",
    "    # Identify categorical columns (object or category dtype)\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    \n",
    "    if not cat_cols:\n",
    "        print(\"No categorical columns found.\")\n",
    "        return\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        s = df[col]\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"Unique values: {s.nunique(dropna=False)}\")\n",
    "        print(\"\\nValue counts:\")\n",
    "        print(s.value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5eoGWCSbzFC"
   },
   "outputs": [],
   "source": [
    "# Check the data\n",
    "show_categoricals(employee_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bfojd-9ek3I"
   },
   "source": [
    "### **2.3 Drop redundant columns** <font color = red>[2 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4tBQCuxbzFC"
   },
   "outputs": [],
   "source": [
    "# Drop redundant columns which are not required for modelling\n",
    "employee_clean.drop(columns='Employee ID', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diL_DCV9bzFC"
   },
   "outputs": [],
   "source": [
    "# Check first few rows of data\n",
    "employee_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j3GERIwML89"
   },
   "source": [
    "## **3. Train-Validation Split** <font color = red>[5 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k6v53qJhWvt"
   },
   "source": [
    "### **3.1 Import required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYg9FxlvbzFH"
   },
   "outputs": [],
   "source": [
    "# Import Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZbrbFe2hex4"
   },
   "source": [
    "### **3.2 Define feature and target variables** <font color = red>[2 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfJfQrc5bzFH"
   },
   "outputs": [],
   "source": [
    "# Put all the feature variables in X\n",
    "_df = employee_clean.copy()\n",
    "X = _df.drop(columns=[\"Attrition\"])\n",
    "# Put the target variable in y\n",
    "y = _df[\"Attrition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "say6J66Bh9mo"
   },
   "source": [
    "### **3.3 Split the data** <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zghEvbRUbzFH"
   },
   "outputs": [],
   "source": [
    "# Split the data into 70% train data and 30% validation data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=100, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldKNxCXogIz2"
   },
   "source": [
    "## **4. EDA on training data** <font color = red>[20 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGyYCt8-ZBR1"
   },
   "source": [
    "### **4.1 Perform univariate analysis** <font color = red>[6 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTdmWrZ1gjmv"
   },
   "source": [
    "Perform univariate analysis on training data for all the numerical columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDDjpMUcZPrp"
   },
   "source": [
    "4.1.1 Select numerical columns from training data <font color = \"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilQNUm3ugqEb"
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx8SQw19ZfXw"
   },
   "source": [
    "4.1.2 Plot distribution of numerical columns <font color = \"red\">[5 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zH5WGU_tgxnL"
   },
   "outputs": [],
   "source": [
    "# Plot all the numerical columns to understand their distribution\n",
    "\n",
    "# Import necessary libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_rows = int(np.ceil(len(num_cols)/3))\n",
    "fig, axes = plt.subplots(fig_rows, 3, figsize=(16, 4*fig_rows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "for i, c in enumerate(num_cols):\n",
    "    sns.histplot(X_train[c], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution: {c}\")\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwJQGBQuZwnC"
   },
   "source": [
    "### **4.2 Perform correlation analysis** <font color=\"red\">[4 Marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RN3ENFP6oRjq"
   },
   "source": [
    "Check the correlation among different numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPFKI4vZoXXw"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix for numerical columns\n",
    "corr = X_train[num_cols].corr(numeric_only=True)\n",
    "\n",
    "# Plot Heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, cmap=\"RdBu_r\", annot=True, center=0)\n",
    "plt.title(\"Correlation matrix (training numerics)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6YQGm9HaKA7"
   },
   "source": [
    "### **4.3 Check class balance** <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_--fA2Dcpjml"
   },
   "source": [
    "Check the distribution of target variable in training set to check class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lp-suUWKpn8h"
   },
   "outputs": [],
   "source": [
    "# Plot a bar chart to check class balance\n",
    "palette = {'Stayed': \"#58BE97\", 'Left': \"#C74D3D\"}\n",
    "ax = sns.countplot(x=y_train, palette=palette)\n",
    "ax.set_title(\"Target distribution (train)\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.bar_label(ax.containers[1])\n",
    "plt.show()\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZ-lppqAaeZM"
   },
   "source": [
    "### **4.4 Perform bivariate analysis** <font color=\"red\">[8 Marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4JjkVzja1kA"
   },
   "source": [
    "Perform bivariate analysis on training data between all the categorical columns and target variable to  analyse how the categorical variables influence the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-rK15xjsv6f"
   },
   "outputs": [],
   "source": [
    "# Plot distribution for each categorical column with target variable\n",
    "cat_cols_train = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "palette = {'Stayed': \"#58BE97\", 'Left': \"#C74D3D\"}\n",
    "\n",
    "for c in cat_cols_train:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    tmp = pd.concat([X_train[[c]].copy(), y_train.rename(\"Attrition\")], axis=1)\n",
    "    sns.countplot(data=tmp, x=c, hue=\"Attrition\", palette=palette)\n",
    "    plt.title(f\"Attrition by {c} (train)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqRcfQ4avArz"
   },
   "source": [
    "## **5. EDA on validation data** <font color = red>[OPTIONAL]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9Iq1uvfa7ro"
   },
   "source": [
    "### **5.1 Perform univariate analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EScWfXFNvAr3"
   },
   "source": [
    "Perform univariate analysis on validation data for all the numerical columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofFfSSPSa0sD"
   },
   "source": [
    "5.1.1 Select numerical columns from validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpkHJm8BvAr3"
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "num_cols = X_test.select_dtypes(include=[np.number]).columns\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZbQ8HzRbDPP"
   },
   "source": [
    "5.1.2 Plot distribution of numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoNtrdrUvAr4"
   },
   "outputs": [],
   "source": [
    "# Plot all the numerical columns to understand their distribution\n",
    "fig_rows = int(np.ceil(len(num_cols)/3))\n",
    "fig, axes = plt.subplots(fig_rows, 3, figsize=(16, 4*fig_rows))\n",
    "axes = np.array(axes).reshape(-1)\n",
    "for i, c in enumerate(num_cols):\n",
    "    sns.histplot(X_test[c], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution: {c}\")\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85zpsiHUvAr4"
   },
   "source": [
    "### **5.2 Perform correlation analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TcmX7ywaWuo"
   },
   "source": [
    "Check the correlation among different numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ylnfMmNqvAr4"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix for numerical columns\n",
    "corr_test = X_test[num_cols].corr(numeric_only=True)\n",
    "\n",
    "# Plot Heatmap of the correlation matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_test, cmap=\"RdBu_r\", annot=True, center=0)\n",
    "plt.title(\"Correlation matrix (training numerics)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fcm5Q1vvAr5"
   },
   "source": [
    "### **5.3 Check class balance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz6Ewijyaa9h"
   },
   "source": [
    "Check the distribution of target variable in validation data to check class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiHsUZnkvAr5"
   },
   "outputs": [],
   "source": [
    "# Plot a bar chart to check class balance\n",
    "palette = {'Stayed': \"#58BE97\", 'Left': \"#C74D3D\"}\n",
    "ax = sns.countplot(x=y_test, palette=palette, hue_order=[\"Stayed\", \"Left\"])\n",
    "ax.set_title(\"Target distribution (test)\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.bar_label(ax.containers[1])\n",
    "plt.show()\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQAoo6aKbcWz"
   },
   "source": [
    "### **5.4 Perform bivariate analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14-qNV44djPP"
   },
   "source": [
    "Perform bivariate analysis on validation data between all the categorical columns and target variable to analyse how the categorical variables influence the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EO4qSIovAr6"
   },
   "outputs": [],
   "source": [
    "# Plot distribution for each categorical column with target variable\n",
    "cat_cols_test = X_test.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "palette = {'Stayed': \"#58BE97\", 'Left': \"#C74D3D\"}\n",
    "\n",
    "for c in cat_cols_test:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    tmp = pd.concat([X_test[[c]].copy(), y_test.rename(\"Attrition\")], axis=1)\n",
    "    sns.countplot(data=tmp, x=c, hue=\"Attrition\", palette=palette)\n",
    "    plt.title(f\"Attrition by {c} (test)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DGrycx6wKmN"
   },
   "source": [
    "## **6. Feature Engineering** <font color = red>[20 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzdF2TmEbzFD"
   },
   "source": [
    "### **6.1 Dummy variable creation** <font color = red>[15 marks]</font>\n",
    "\n",
    "\n",
    "The next step is to deal with the categorical variables present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vG9HZkVe3Lk"
   },
   "source": [
    "6.1.1 Identify categorical columns where dummy variables are required <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dUFEFVGbzFD"
   },
   "outputs": [],
   "source": [
    "# Check the categorical columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLpmkPp6e-1j"
   },
   "source": [
    "6.1.2 Create dummy variables for independent columns in training set <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5vk968HbzFE"
   },
   "outputs": [],
   "source": [
    "# Create dummy variables using the 'get_dummies' for independent columns\n",
    "X_train_dummy = pd.get_dummies(X_train[cat_cols], drop_first=True).astype(int)\n",
    "# Add the results to the master DataFrame\n",
    "X_train = pd.concat([X_train, X_train_dummy], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJFSbVR9uYjT"
   },
   "source": [
    "Now, drop the original categorical columns and check the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-U8OPvKyOs-"
   },
   "outputs": [],
   "source": [
    "# Drop the original categorical columns and check the DataFrame\n",
    "X_train.drop(columns=cat_cols, inplace=True, errors='ignore')\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqMXYxpowQc6"
   },
   "source": [
    "6.1.3 Create dummy variables for independent columns in validation set <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ib8Zdq6Ruuj_"
   },
   "outputs": [],
   "source": [
    "# Create dummy variables using the 'get_dummies' for independent columns\n",
    "X_test_dummy = pd.get_dummies(X_test[cat_cols], drop_first=True).astype(int)\n",
    "\n",
    "# Add the results to the master DataFrame\n",
    "X_test = pd.concat([X_test, X_test_dummy], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RDKGX9N4qLq"
   },
   "source": [
    "Now, drop the original categorical columns and check the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dh6Mk_JvyUkz"
   },
   "outputs": [],
   "source": [
    "# Drop categorical columns and check the DataFrame\n",
    "X_test.drop(columns=cat_cols, inplace=True, errors='ignore')\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zesgvCjzPHba"
   },
   "source": [
    "6.1.4 Create DataFrame for dependent column in both training and validation set <font color = \"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2JTJRaa-qem"
   },
   "outputs": [],
   "source": [
    "# Convert y_train and y_validation to DataFrame to create dummy variables\n",
    "y_train = pd.DataFrame({'Attrition':y_train})\n",
    "y_test = pd.DataFrame({'Attrition':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QN-k-IHPX5s"
   },
   "source": [
    "6.1.5 Create dummy variables for dependent column in training set <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OfrtF4a59LNK"
   },
   "outputs": [],
   "source": [
    "# Create dummy variables using the 'get_dummies' for dependent column\n",
    "y_train = pd.get_dummies(y_train).astype(int)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p_zQEx4w-OU"
   },
   "source": [
    "6.1.6 Create dummy variable for dependent column in validation set <font color = \"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mWed8j5xOS7"
   },
   "outputs": [],
   "source": [
    "# Create dummy variables using the 'get_dummies' for dependent column\n",
    "y_test = pd.get_dummies(y_test).astype(int)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ej5eSB4PiCx"
   },
   "source": [
    "6.1.7 Drop redundant columns <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxYmaNE5Aoas"
   },
   "outputs": [],
   "source": [
    "# Drop redundant columns from both train and validation\n",
    "y_train.drop(columns='Attrition_Left', inplace=True, errors='ignore')\n",
    "y_test.drop(columns='Attrition_Left', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zL0XwIAbzFH"
   },
   "source": [
    "### **6.2 Feature scaling** <font color = red>[5 marks]</font>\n",
    "\n",
    "Apply feature scaling to the numeric columns to bring them to a common range and ensure consistent scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2Di6iSjU41j"
   },
   "source": [
    "6.2.1 Import required libraries <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhsJWDc4bzFH"
   },
   "outputs": [],
   "source": [
    "# Import the necessary scaling tool from scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RE9MEgQVAg8"
   },
   "source": [
    "6.2.2 Scale the numerical features <font color=\"red\">[4 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCV2mKErbzFH"
   },
   "outputs": [],
   "source": [
    "# Scale the numeric features present in the training set\n",
    "X_train_scaled = X_train.copy()\n",
    "X_train_scaled[num_cols] = scaler.fit_transform(X_train_scaled[num_cols])\n",
    "display(X_train_scaled.head())\n",
    "\n",
    "# Scale the numerical features present in the validation set\n",
    "X_test_scaled = X_test.copy()\n",
    "X_test_scaled[num_cols] = scaler.transform(X_test_scaled[num_cols])\n",
    "display(X_test_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFIOMdJqBRyw"
   },
   "source": [
    "## **7. Model Building** <font color = red>[40 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VrBY3MpbzFI"
   },
   "source": [
    "### **7.1 Feature selection** <font color = red>[5 marks]</font>\n",
    "\n",
    "As there are a lot of variables present in the data, Recursive Feature Elimination (RFE) will be used to select the most influential features for building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "927evRo1VO4e"
   },
   "source": [
    "7.1.1 Import required libraries <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1uKag9gbzFI"
   },
   "outputs": [],
   "source": [
    "# Import 'LogisticRegression' and create a LogisticRegression object\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_flTl2qVS1t"
   },
   "source": [
    "7.1.2 Import RFE  and select 15 variables <font color=\"red\">[3 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMAKjGQAbzFI"
   },
   "outputs": [],
   "source": [
    "# Import RFE and select 15 variables\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "rfe = RFE(estimator=log_reg, n_features_to_select=15)\n",
    "rfe.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znuj-wmubzFJ"
   },
   "outputs": [],
   "source": [
    "# Display the features selected by RFE\n",
    "X_train_scaled.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfSLJwIFbzFJ"
   },
   "source": [
    "7.1.3 Store the selected features <font color=\"red\">[1 Mark]</font>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CblOqMSzbzFJ"
   },
   "outputs": [],
   "source": [
    "# Put columns selected by RFE into variable 'col'\n",
    "col = X_train_scaled.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMGvl2H1bzFJ"
   },
   "source": [
    "### **7.2 Building Logistic Regression Model** <font color = red>[20 marks]</font>\n",
    "\n",
    "Now that you have selected the variables through RFE, use these features to build a logistic regression model with statsmodels. This will allow you to assess the statistical aspects, such as p-values and VIFs, which are important for checking multicollinearity and ensuring that the predictors are not highly correlated with each other, as this could distort the model's coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9t-NQ2KjHv6"
   },
   "source": [
    "7.2.1 Select relevant columns on training set <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLHDHKfSbzFJ"
   },
   "outputs": [],
   "source": [
    "# Select only the columns selected by RFE\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLukEfebbzFJ"
   },
   "outputs": [],
   "source": [
    "# View the training data\n",
    "X_train_scaled[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ikq-mNcWjZ7u"
   },
   "source": [
    "7.2.2 Add constant to training set <font color = \"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNcomiZdbzFK"
   },
   "outputs": [],
   "source": [
    "# Import statsmodels and add constant to training set\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_train_const = sm.add_constant(X_train_scaled[col])\n",
    "X_train_const.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbOjP0UJjmJr"
   },
   "source": [
    "7.2.3 Fit logistic regression model <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTQHS0e7bzFK"
   },
   "outputs": [],
   "source": [
    "# Fit a logistic regression model on X_train after adding a constant and output the summary\n",
    "log_reg.fit(X_train_const, y_train)\n",
    "\n",
    "print(\"Classes:\", log_reg.classes_)\n",
    "print(\"Intercept (bias):\", log_reg.intercept_)\n",
    "print(\"Coefficients:\", log_reg.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyLcup7xbzFK"
   },
   "source": [
    "**Model Interpretation**\n",
    "\n",
    "The output summary table  will provide the features used for building model along with coefficient of each of the feature and their p-value. The p-value in a logistic regression model is used to assess the statistical significance of each coefficient. Lesser the p-value, more significant the feature is in the model.\n",
    "\n",
    "A positive coefficient will indicate that an increase in the value of feature would increase the odds of the event occurring. On the other hand, a negative coefficient means the opposite, i.e,  an increase in the value of feature would decrease the odds of the event occurring.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LICa4vdcj0nP"
   },
   "source": [
    "7.2.4 Evaluate VIF of features <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCHI-Z-UbzFK"
   },
   "outputs": [],
   "source": [
    "# Import 'variance_inflation_factor'\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ute-cXihbzFL"
   },
   "outputs": [],
   "source": [
    "# Make a VIF DataFrame for all the variables present\n",
    "X_vif = X_train_const.drop(columns='const')\n",
    "\n",
    "def make_vif_df(X):\n",
    "    vif_values = []\n",
    "    for i in range(X.shape[1]):\n",
    "        vif = variance_inflation_factor(X.values, i)\n",
    "        vif_values.append(vif)\n",
    "    \n",
    "    return (pd.DataFrame({\n",
    "                \"feature\": X.columns,\n",
    "                \"VIF\": vif_values\n",
    "            })\n",
    "            .sort_values(\"VIF\", ascending=False)\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "make_vif_df(X_vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iDznUNbbzFL"
   },
   "source": [
    "Proceed to the next step if p-values and VIFs are within acceptable ranges.  If you observe high p-values or VIFs, create new cells to drop the features and retrain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVrg9XDGkC1p"
   },
   "source": [
    "7.2.5 Make predictions on training set <font color = \"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq545BKdbzFL"
   },
   "outputs": [],
   "source": [
    "# Predict the probabilities on the training set\n",
    "y_train_proba = log_reg.predict_proba(X_train_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7M8kndokRo3"
   },
   "source": [
    "7.2.6 Format the prediction output <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqSLANvybzFL"
   },
   "outputs": [],
   "source": [
    "# Reshape it into an array\n",
    "y_train_proba = np.array(y_train_proba)\n",
    "y_train_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B__pQ9mBbzFM"
   },
   "source": [
    "7.2.7 Create a DataFrame with the actual stayed flag and the predicted probabilities <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YcNu7FEbzFM"
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame containing the actual stayed flag and the probabilities predicted by the model\n",
    "y_train_pred_df = pd.concat([\n",
    "    y_train, \n",
    "    pd.DataFrame({'Stayed_Proba':y_train_proba[:, 1]}, index=y_train.index)\n",
    "    ], axis=1)\n",
    "y_train_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShtZ7a30bzFM"
   },
   "source": [
    "7.2.8 Create a new column 'Predicted' with 1 if predicted probabilities are greater than 0.5 else 0 <font color = \"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_V3Pq0k4bzFM"
   },
   "outputs": [],
   "source": [
    "# Create a new column 'Predicted' with 1 if predicted probabilities are greater than 0.5 else 0\n",
    "y_train_pred_df['Prediction'] = (y_train_pred_df['Stayed_Proba'] > 0.5).astype(int)\n",
    "y_train_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6mNG69ybzFL"
   },
   "source": [
    "**Evaluation of performance of Model**\n",
    "\n",
    "Evaluate the performance of the model based on the predictions made on the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl_XslnfbzFM"
   },
   "source": [
    "7.2.9 Check the accuracy of the model based on the predictions made on the training set <font color = \"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jhA1kq_dbzFM"
   },
   "outputs": [],
   "source": [
    "# Import metrics from sklearn for evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# Check the overall accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_train_pred_df['Attrition_Stayed'], y_train_pred_df['Prediction'])\n",
    "# y_train_true = y_train_pred_df['Attrition_Stayed']\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIo_9vjSbzFM"
   },
   "source": [
    "7.2.10 Create a confusion matrix based on the predictions made on the training set <font color=\"red\">[1 mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_1DsDkjbzFN"
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "\n",
    "confusion_matrix(y_train_pred_df['Attrition_Stayed'], y_train_pred_df['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_pred_df['Attrition_Stayed'], y_train_pred_df['Prediction']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhUC-9xrpv1L"
   },
   "source": [
    "7.2.11 Create variables for true positive, true negative, false positive and false negative <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lK9ZbfjIbzFN"
   },
   "outputs": [],
   "source": [
    "# Create variables for true positive, true negative, false positive and false negative\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_pred_df['Attrition_Stayed'], y_train_pred_df['Prediction']).ravel().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teRSbuOSbzFN"
   },
   "source": [
    "7.2.12 Calculate sensitivity and specificity of model  <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-G-5XVNJbzFN"
   },
   "outputs": [],
   "source": [
    "# Calculate sensitivity\n",
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A24XTKXnEFYH"
   },
   "outputs": [],
   "source": [
    "# Calculate specificity\n",
    "tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDuRAKYgbzFO"
   },
   "source": [
    "7.2.13 Calculate precision and recall of model <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwzXNQqYbzFO"
   },
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "tp / (tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCNnfrQIbzFO"
   },
   "outputs": [],
   "source": [
    "# Calculate recall\n",
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IThpUyeKbzFO"
   },
   "source": [
    "### **7.3 Find the optimal cutoff** <font color = red>[15 marks]</font>\n",
    "\n",
    "Find the optimal cutoff to improve model performance. While a default threshold of 0.5 was used for initial evaluation, optimising this threshold can enhance the model's performance.\n",
    "\n",
    "First, plot the ROC curve and check AUC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6P4KhA_pH-P"
   },
   "source": [
    "7.3.1 Plot ROC curve <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QAnutZNbzFO"
   },
   "outputs": [],
   "source": [
    "# Define ROC function\n",
    "fpr, tpr, thresholds = roc_curve(y_train_pred_df['Attrition_Stayed'], y_train_pred_df['Stayed_Proba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hkCvgfEbzFO"
   },
   "outputs": [],
   "source": [
    "# Call the ROC function\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, label=f\"LogReg ROC (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], \"r--\", label=\"Random\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "J = tpr - fpr  # since J = TPR + (1 - FPR) - 1 = TPR - FPR\n",
    "best_idx = np.argmax(J)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIbs06KDbzFO"
   },
   "source": [
    "**Sensitivity and Specificity tradeoff**\n",
    "\n",
    "Check sensitivity and specificity tradeoff to find the optimal cutoff point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1II4CXInpfuZ"
   },
   "source": [
    "7.3.2 Predict on training set at various probability cutoffs <font color=\"red\">[1 Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-74M5fbIbzFO"
   },
   "outputs": [],
   "source": [
    "# Predict on training data by creating columns with different probability cutoffs to explore the impact of cutoff on model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = np.round(np.linspace(0.05, 0.95, 19), 2)\n",
    "\n",
    "df = pd.DataFrame({'y_true': y_train_pred_df['Attrition_Stayed'], 'y_score': y_train_pred_df['Stayed_Proba']})\n",
    "for c in cutoffs:\n",
    "    df[f'pred_c{c:.2f}'] = (df['y_score'] >= c).astype(int)\n",
    "\n",
    "rows = []\n",
    "for c in cutoffs:\n",
    "    y_pred = df[f'pred_c{c:.2f}']\n",
    "    acc = accuracy_score(df['y_true'], y_pred)\n",
    "    prec = precision_score(df['y_true'], y_pred, zero_division=0)\n",
    "    rec = recall_score(df['y_true'], y_pred, zero_division=0)\n",
    "    f1 = f1_score(df['y_true'], y_pred, zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(df['y_true'], y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    rows.append({\n",
    "        'cutoff': c,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'specificity': specificity,\n",
    "        'f1': f1,\n",
    "        'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).sort_values('cutoff').reset_index(drop=True)\n",
    "\n",
    "roc_auc = roc_auc_score(df['y_true'], df['y_score'])\n",
    "fpr, tpr, thresholds = roc_curve(df['y_true'], df['y_score'])\n",
    "\n",
    "print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DA0BEORfp_nW"
   },
   "source": [
    "7.3.3 Plot for accuracy, sensitivity, specificity at different probability cutoffs <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "An1cy6CJbzFP"
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame to see the values of accuracy, sensitivity, and specificity at different values of probability cutoffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P725dvI6bzFP"
   },
   "outputs": [],
   "source": [
    "# Plot accuracy, sensitivity, and specificity at different values of probability cutoffs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnCc19DyqUSt"
   },
   "source": [
    "7.3.4 Create a column for final prediction based on the optimal cutoff <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLyy-eawbzFP"
   },
   "outputs": [],
   "source": [
    "# Create a column for final prediction based on the optimal cutoff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRyaBXdEqryR"
   },
   "source": [
    "7.3.5 Calculate model's accuracy <font color=\"red\">[1Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dr5PFXZMbzFP"
   },
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfgLjKNYrDr8"
   },
   "source": [
    "7.3.6 Create confusion matrix <font color=\"red\">[1Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNCQmHAgbzFQ"
   },
   "outputs": [],
   "source": [
    "# Create the confusion matrix once again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE80CYQ4rMCM"
   },
   "source": [
    "7.3.7 Create variables for true positive, true negative, false positive and false negative <font color=\"red\">[1Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l53itzjzbzFQ"
   },
   "outputs": [],
   "source": [
    "# Create variables for true positive, true negative, false positive and false negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0drkvl0drY2t"
   },
   "source": [
    "7.3.8 Calculate sensitivity and specificity of the model <font color=\"red\">[1Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtXYfD1mbzFQ"
   },
   "outputs": [],
   "source": [
    "# Calculate Sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4Meaz7RbzFR"
   },
   "outputs": [],
   "source": [
    "# Calculate Specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBm4vL_2rhgF"
   },
   "source": [
    "7.3.9 Calculate precision and recall of the model <font color=\"red\">[1Mark]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnQgXuQCbzFR"
   },
   "outputs": [],
   "source": [
    "# Calculate Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4HQbkZhbzFS"
   },
   "outputs": [],
   "source": [
    "# Calculate Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxmGPpwpbzFS"
   },
   "source": [
    "**Precision and Recall tradeoff**\n",
    "\n",
    "Check optimal cutoff value by plotting precision-recall curve, and adjust the cutoff based on the precision and recall tradeoff if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1gibcCMbzFS"
   },
   "outputs": [],
   "source": [
    "# Import precision-recall curve function\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MV-oGIWHbzFT"
   },
   "outputs": [],
   "source": [
    "# Check actual and predicted values from initial model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dV_jCG6rykd"
   },
   "source": [
    "7.3.10 Plot precision-recall curve <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da902sgzbzFT"
   },
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6fCFjfubzFT"
   },
   "source": [
    "## **8. Prediction and Model Evaluation** <font color = red>[30 marks]</font>\n",
    "\n",
    "Use the model from the previous step to make predictions on the validation set with the optimal cutoff. Then evaluate the model's performance using metrics such as accuracy, sensitivity, specificity, precision, and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEY3dsXNbzFU"
   },
   "source": [
    "### **8.1 Make predictions over validation set** <font color = red>[15 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PSHBQ5qbzFU"
   },
   "source": [
    "8.1.1 Select relevant features for validation set <font color=\"red\">[2 Marks]</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9n1aTakbzFU"
   },
   "outputs": [],
   "source": [
    "# Select the relevant features for validation set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNGNgwNrsXf9"
   },
   "source": [
    "8.1.2 Add constant to X_validation <font color=\"red\">[2 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y_CGwNWbzFU"
   },
   "outputs": [],
   "source": [
    "# Add constant to X_validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSl-wRsRshW_"
   },
   "source": [
    "8.1.3 Make predictions over validation set <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7tRbWcQbzFU"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the validation set and store it in the variable 'y_validation_pred'\n",
    "\n",
    "# View predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONdgg12qtJSs"
   },
   "source": [
    "8.1.4 Create DataFrame with actual values and predicted values for validation set <font color=\"red\">[5 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxhXWRVMbzFV"
   },
   "outputs": [],
   "source": [
    "# Convert 'y_validation_pred' to a DataFrame 'predicted_probability'\n",
    "\n",
    "# Convert 'y_validation' to DataFrame 'actual'\n",
    "\n",
    "# Remove index from both DataFrames 'actual' and 'predicted_probability' to append them side by side\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HrzB4MRtlGB"
   },
   "source": [
    "8.1.5 Predict final prediction based on the cutoff value <font color=\"red\">[3 Marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCshcMgnbzFW"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the validation set using the optimal cutoff and store it in a column 'final_prediction'\n",
    "\n",
    "# Check the DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMSL-5-pbzFW"
   },
   "source": [
    "### **8.2 Calculate accuracy of the model** <font color = red>[2 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bwRF3dn6bzFW"
   },
   "outputs": [],
   "source": [
    "# Calculate the overall accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tNEp8_bbzFW"
   },
   "source": [
    "### **8.3 Create confusion matrix and create variables for true positive, true negative, false positive and false negative** <font color = red>[5 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnaOhweTbzFW"
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbnOzs9VbzFW"
   },
   "outputs": [],
   "source": [
    "# Create variables for true positive, true negative, false positive and false negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfnlV8xobzFW"
   },
   "source": [
    "### **8.4 Calculate sensitivity and specificity** <font color = red>[4 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-M3WRwH9bzFW"
   },
   "outputs": [],
   "source": [
    "# Calculate sensitivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mAf35xDnbzFX"
   },
   "outputs": [],
   "source": [
    "# Calculate specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsUKjAT0bzFX"
   },
   "source": [
    "### **8.5 Calculate precision and recall** <font color = red>[4 marks]</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19C3vTO3bzFX"
   },
   "outputs": [],
   "source": [
    "# Calculate precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vvyUapBfbzFX"
   },
   "outputs": [],
   "source": [
    "# Calculate recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hmyrh46kbzFY"
   },
   "source": [
    "## Conclusion\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "mike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
